JPMorgan Chase is temporarily clamping down on the use of ChatGPT among its employees, as the buzzy AI chatbot explodes in popularity.  The biggest US bank has restricted its use among global staff, according to a person familiar with the matter. The decision was taken not because of a particular issue, but to accord with limits on third-party software due to compliance concerns, the person said. JPMorgan Chase declined to comment.  ChatGPT was released to the public in late November by artificial intelligence research company Open AI. Since then, the much-hyped tool has been used to turn written prompts into convincing academic essays and creative scripts as well as trip itineraries and computer code. Adoption has skyrocketed. UBS estimated that ChatGPT reached 100 million monthly active users in January, two months after its launch. That would make it the fastest-growing online application in history, according to the Swiss bank's analysts. The viral success of ChatGPT has kickstarted a frantic competition among tech companies to rush AI products to market. Google recently unveiled its ChatGPT competitor, which it's calling Bard, while Microsoft, an investor in Open AI, debuted its Bing AI chatbot to a limited pool of testers. But the releases have boosted concerns about the technology. Demos of both Google and Microsoft's tools have been called out for producing factual errors. Microsoft, meanwhile, is trying to rein in its Bing chatbot after users reported troubling responses, including confrontational remarks and dark fantasies. Some businesses have encouraged workers to incorporate ChatGPT into their daily work. But others worry about the risks. The banking sector, which deals with sensitive client information and is closely watched by government regulators, has extra incentive to tread carefully. Schools are also restricting ChatGPT due to concerns it could be used to cheat on assignments. New York City public schools banned it in January.

Next time you're wondering how to respond to a colleague on Slack, you may be able to ask ChatGPT for help. Salesforce, the company behind Slack, announced Tuesday that it's partnering with OpenAI to launch a ChatGPT app for the workplace messaging platform. The new tool will use the AI chatbot to "deliver instant conversation summaries, research tools, and writing assistance directly in Slack," according to Salesforce. "There couldn't be a more natural fit," Noah Desai Weiss, Slack's chief product officer, wrote in the release. Workers will be able to use the tech to get instantaneous summaries of conversations, as well as tools to assist in faster research and help with drafting messages to coworkers. The tool will pull from information found within Slack's channel archives as well as the vast trove of online data that ChatGPT has been trained on.  The tool is currently in beta testing. Companies interested in joining can fill out a form through the website of ChatGPT creator OpenAI to be added to the waitlist. The move to bring ChatGPT to Slack is just the latest example of the AI chatbot finding its way into more services. Last week, OpenAI opened up access to the tool to third-party businesses. Instacart, Snap and tutor app Quizlet were among the early partners experimenting with adding ChatGPT. OpenAI publicly released ChatGPT in late November and stunned many users with the tool's impressive ability to generate original essays, stories and song lyrics in response to user prompts. The initial wave of attention on the tool helped renew an arms race among tech companies to develop and deploy similar AI tools in their products. Microsoft recently unveiled an AI-powered revamp of Bing powered by technology from OpenAI. Later this month, Microsoft is set to hold another event to discuss how AI could help with "reinventing productivity" and "the future of work."  But the slack integration could be one of the biggest real world tests yet not just for whether ChatGPT can be entertaining, but also whether it can live up to the promise of making people more productive. It may also test the risks of professionals relying on AI chatbot tools, which have shared inaccurate or incendiary responses, in a workplace setting.

Google on Monday unveiled a new chatbot tool dubbed "Bard" in an apparent bid to compete with the viral success of ChatGPT. Sundar Pichai, CEO of Google and parent company Alphabet, said in a blog post that Bard will be opened up to "trusted testers" starting Monday, with plans to make it available to the public "in the coming weeks."  Like ChatGPT, which was released publicly in late November by AI research company OpenAI, Bard is built on a large language model. These models are trained on vast troves of data online in order to generate compelling responses to user prompts.  "Bard seeks to combine the breadth of the world's knowledge with the power, intelligence and creativity of our large language models," Pichai wrote. "It draws on information from the web to provide fresh, high-quality responses." The announcement comes as Google's core product -- online search -- is widely thought to be facing its most significant risk in years. In the two months since it launched to the public, ChatGPT has been used to generate essays, stories and song lyrics, and to answer some questions one might previously have searched for on Google. The immense attention on ChatGPT has reportedly prompted Google's management to declare a "code red" situation for its search business. In a tweet last year, Paul Buchheit, one of the creators of Gmail, forewarned that Google "may be only a year or two away from total disruption" due to the rise of AI.  Microsoft, which has confirmed plans to invest billions OpenAI, has already said it would incorporate the tool into some of its products -- and it is rumored to be planning to integrate it into its search engine, Bing. Microsoft on Tuesday is set to hold a news event at its Washington headquarters, the topic of which has yet to be announced. Microsoft publicly announced the event shortly after Google's AI news dropped on Monday.  The underlying technology that supports Bard has been around for some time, though not widely available to the public. Google unveiled its Language Model for Dialogue Applications (or LaMDA) some two years ago, and said Monday that this technology will power Bard. LaMDA made headlines late last year when a former Google engineer claimed the chatbot was "sentient." His claims were widely criticized in the AI community.  In the post Monday, Google offered the example of a user asking Bard to explain new discoveries made by NASA's James Webb Space Telescope in a way that a 9-year-old might find interesting. Bard responds with conversational bullet-points. The first one reads: "In 2023, The JWST spotted a number of galaxies nicknamed 'green peas.' They were given this name because they are small, round, and green, like peas." Bard can be used to plan a friend's baby shower, compare two Oscar-nominated movies or get lunch ideas based on what's in your fridge, according to the post from Google.  Pichai also said Monday that AI-powered tools will soon begin rolling out on Google's flagship Search tool.  "Soon, you'll see AI-powered features in Search that distill complex information and multiple perspectives into easy-to-digest formats, so you can quickly understand the big picture and learn more from the web," Pichai wrote, "whether that's seeking out additional perspectives, like blogs from people who play both piano and guitar, or going deeper on a related topic, like steps to get started as a beginner." If Google does move more in the direction of incorporating an AI chatbot tool into search, it could come with some risks. Because these tools are trained on data online, experts have noted they have the potential to perpetuate biases and spread misinformation.  "It's critical," Pichai wrote in his post, "that we bring experiences rooted in these models to the world in a bold and responsible way.

Jeff Maggioncalda, the CEO of online learning provider Coursera, said that when he first tried ChatGPT, he was "dumbstruck." Now, it's part of his daily routine. He uses the powerful new AI chatbot tool to bang out emails. He uses it to craft speeches "in a friendly, upbeat, authoritative tone with mixed cadence." He even uses it to help break down big strategic questions — such as how Coursera should approach incorporating artificial intelligence tools like ChatGPT into its platform. "I use it as a writing assistant and as a thought partner," Maggioncalda told CNN. Maggioncalda is one of thousands of business leaders, politicians and academics gathered in Davos, Switzerland this week for the World Economic Forum. On the agenda is an array of pressing issues weighing on the global economy, from the energy crisis to the war in Ukraine and the transformation of trade. But what many can't stop talking about is ChatGPT. The tool, which artificial intelligence research company OpenAI made available to the general public late last year, has sparked conversations about how "generative AI" services — which can turn prompts into original essays, stories, songs and images after training on massive online datasets — could radically transform how we live and work.  Some claim it will put artists, tutors, coders, and writers (yes, even journalists) out of a job. Others are more optimistic, postulating that it will allow employees to tackle to-do lists with greater efficiency or focus on higher-level tasks.  It's a debate that's captivated many C-suite leaders, often after they tested the tool themselves. Christian Lanng, CEO of digital supply chain platform Tradeshift, said he was blown away by the capabilities displayed by ChatGPT, even after years of exposure to Silicon Valley hype. He's also used the platform to write emails and claims no one has noticed the difference. He even had it perform some accounting work, a service for which Tradeshift currently employs an expensive professional services firm. To date, ChatGPT has mostly been treated as a curiosity and a harbinger of what's to come. It relies on OpenAI's GPT-3.5 language model, which is already out of date; the more advanced GPT-4 version is in the works and could be released this year.  Critics — of which there are many — are quick to point out that it makes mistakes, is painfully neutral and displays a clear lack of human empathy. One tech news publication, for example, was forced to issue several significant corrections for an article written by ChatGPT. And New York City public schools have banned students and teachers from using it. Yet the software, or similar programs from competitors, could soon take the business world by storm. Microsoft, an investor in OpenAI, announced this week that the company's tools — including GPT-3.5, programming assistant Codex and image generator DALL-E 2 — are now generally available to business clients in a package called Azure OpenAI Service. ChatGPT is being added soon. "I see these technologies acting as a copilot, helping people do more with less," Microsoft CEO Satya Nadella told an audience in Davos this week. Maggioncalda has a similar perspective. He wants to integrate generative AI into Coursera's offering this year, seeing an opportunity to make learning more interactive for students who don't have access to in-person classroom instruction or one-on-one time with subject matter experts. He acknowledges challenges such as preventing cheating and ensuring accuracy need to be addressed. And he's worried that increasing use of generative AI may not be wholly good for society — people may become less agile thinkers, for example, since the act of writing can be helpful to process complex ideas and hone takeaways.  Still, he sees the need to move quickly. "Anybody who doesn't use this will shortly be at a severe disadvantage. Like, shortly. Like, very soon," Maggioncalda said. "I'm just thinking about my cognitive ability with this tool. Versus before, it's a lot higher, and my efficiency and productivity is way higher.

ChatGPT is smart enough to pass prestigious graduate-level exams -- though not with particularly high marks. The powerful new AI chatbot tool recently passed law exams in four courses at the University of Minnesota and another exam at University of Pennsylvania's Wharton School of Business, according to professors at the schools. To test how well ChatGPT could generate answers on exams for the four courses, professors at the University of Minnesota Law School recently graded the tests blindly. After completing 95 multiple choice questions and 12 essay questions, the bot performed on average at the level of a C+ student, achieving a low but passing grade in all four courses.  ChatGPT fared better during a business management course exam at Wharton, where it earned a B to B- grade. In a paper detailing the performance, Christian Terwiesch, a Wharton business professor, said ChatGPT did "an amazing job" at answering basic operations management and process-analysis questions but struggled with more advanced prompts and made "surprising mistakes" with basic math.  "These mistakes can be massive in magnitude," he wrote.  The test results come as a growing number of schools and teachers express concerns about the immediate impact of ChatGPT on students and their ability to cheat on assignments. Some educators are now moving with remarkable speed to rethink their assignments in response to ChatGPT, even as it remains unclear how widespread use is of the tool among students and how harmful it could really be to learning.  Since it was made available in late November, ChatGPT has been used to generate original essays, stories and song lyrics in response to user prompts. It has drafted research paper abstracts that fooled some scientists. Some CEOs have even used it to write emails or do accounting work. ChatGPT is trained on vast amounts of online data in order to generate responses to user prompts. While it has gained traction among users, it has also raised some concerns, including about inaccuracies and its potential to perpetuate biases and spread misinformation.  Jon Choi, one of the University of Minnesota law professors, told CNN the goal of the tests was to explore ChatGPT's potential to assist lawyers in their practice and to help students in exams, whether or not it's permitted by their professors, because the questions often mimic the writing lawyers do in real life. "ChatGPT struggled with the most classic components of law school exams, such as spotting potential legal issues and deep analysis applying legal rules to the facts of a case," Choi said. "But ChatGPT could be very helpful at producing a first draft that a student could then refine." He argues human-AI collaboration is the most promising use case for ChatGPT and similar technology. "My strong hunch is that AI assistants will become standard tools for lawyers in the near future, and law schools should prepare their students for that eventuality," he said. "Of course, if law professors want to continue to test simple recall of legal rules and doctrines, they'll need to put restrictions in place like banning the internet during exams to enforce that." Likewise, Wharton's Terwiesch found the chatbot was "remarkably good" at modifying its answers in response to human hints, such as reworking answers after pointing out an error, suggesting the potential for people to work together with AI.  In the short-term, however, discomfort remains with whether and how students should use ChatGPT. Public schools in New York City and Seattle, for example, have already banned students and teachers from using ChatGPT on the district's networks and devices.  Considering ChatGPT performed above average on his exam, Terwiesch told CNN he agrees restrictions should be put in place for students while they're taking tests.  "Bans are needed," he said. "After all, when you give a medical doctor a degree, you want them to know medicine, not how to use a bot. The same holds for other skill certification, including law and business." But Terwiesch believes this technology still ultimately has a place in the classroom. "If all we end up with is the same educational system as before, we have wasted an amazing opportunity that comes with ChatGPT," he said.


Microsoft on Monday confirmed it is making a "multibillion dollar" investment in OpenAI,  the company behind the viral new AI chatbot tool called ChatGPT. Microsoft, an early investor in OpenAI, said it plans to expand its existing partnership with the company as part of a greater effort to add more artificial intelligence to its suite of products. In a separate blog post, OpenAI said the multi-year investment will be used to "develop AI that is increasingly safe, useful, and powerful." "We formed our partnership with OpenAI around a shared ambition to responsibly advance cutting-edge AI research and democratize AI as a new technology platform," Satya Nadella, Microsoft's CEO, said in a statement.  The deepening partnership between the two companies has the potential to supercharge OpenAI's ambitious projects, including ChatGPT,  which has captured the attention of -- and sometimes sparked concerns from -- academics, business leaders and tech enthusiasts with its ability to create  provide lengthy and thorough responses to user prompts and questions. The investment could also catapult Microsoft as an AI leader and ultimately pave the way for the company to incorporate ChatGPT into some of its hallmark applications, such as Word, PowerPoint and Outlook.  As a result of its existing exclusive deal with OpenAI, Microsoft recently said it would soon add ChatGPT features to to its cloud computing service, Azure. If ChatGPT becomes available on that service, businesses could use the tools directly within its apps and services, too. The investment comes days after Microsoft announced plans to lay off 10,000 employees as part of broader cost-cutting measures. Nadella said at the time that the company will continue to invest in "strategic areas for our future" and pointed to advances in AI as "the next major wave" of computing.  A bet on the future of AI Since OpenAI opened up access to ChatGPT in late November, it has been used to write articles (with more than a couple factual inaccuracies) for at least one news publication; penned lyrics in the style of various artists (one of whom later responded, "this song sucks") and drafted research paper abstracts that fooled some scientists.  Some CEOs have also already used the platform to write emails or do some accounting work.  OpenAI is also the company behind DALL-E, which generates a seemingly limitless range of images in response to prompts from users. Both DALL-E and ChatGPT are trained on vast amounts of data in order to generate content.  But there are some risks for Microsoft and OpenAI here.  While these products have gained traction among users, they've also raised some concerns, including about their potential to perpetuate biases and spread misinformation. At the same time, a growing number of schools and teachers are concerned about the immediate impact of ChatGPT on students and their ability to cheat on assignments. That could potentially create "much negative publicity" for the companies associated with these tools, said David Lobina, an artificial intelligence analyst at ABI Research. The opportunity is immense, however, and could boost Microsoft's position in the growing arms race over artificial intelligence, and provide a helpful boost to OpenAI in the process.  "OpenAI is looking to monetize their systems, considering the huge compute costs of creating these models," Lobina told CNN ahead of the announcement Monday. "Their partnership with Microsoft can be an easy way to do so.

Google is opening up access to Bard, its new AI chatbot tool that directly competes with ChatGPT.  Starting Tuesday, users can join a waitlist to gain access to Bard, which promises to help users outline and write essay drafts, plan a friend's baby shower, and get lunch ideas based on what's in the fridge. A company representative told CNN it will be a separate, complementary experience to Google Search, and users can also visit Search to check its responses or sources. Google said in a blog post it plans to "thoughtfully" add large language models to search "in a deeper way" at a later time. Google said it will start rolling out the tool in the United States and United Kingdom, and plans to expand it to more countries and languages in the future. The news comes as Google, Microsoft, Facebook and other tech companies race to develop and deploy AI-powered tools in the wake of the recent, viral success of ChatGPT. Last week, Google announced it is also bringing AI to its productivity tools, including Gmail, Sheets and Docs. Shortly after, Microsoft announced a similar AI upgrade to its productivity tools. Google unveiled Bard last month in a demo that was later called out for providing an inaccurate response to a question about a telescope. Shares of Google's parent company Alphabet fell 7.7% that day, wiping $100 billion off its market value. Like ChatGPT, which was released publicly in late November by AI research company OpenAI, Bard is built on a large language model. These models are trained on vast troves of data online in order to generate compelling responses to user prompts. The immense attention on ChatGPT reportedly prompted Google's management to declare a "code red" situation for its search business.  But Bard's blunder highlighted the challenge Google and other companies face with integrating the technology into their core products. Large language models can present a handful of issues, such as perpetuating biases, being factually incorrect and responding in an aggressive manner. Google acknowledged in the blog post Tuesday that AI tools are "not without their faults." The company said it continues to use human feedback to improve its systems and add new "guardrails, like capping the number of exchanges in a dialogue, to try to keep interactions helpful and on topic." Last week, OpenAI released GPT-4, the next-generation version of the technology that powers ChatGPT and Microsoft's new Bing browser, with similar safeguards. In the first day after it was unveiled, GPT-4 stunned many users in early tests and a company demo with its ability to draft lawsuits, pass standardized exams and build a working website from a hand-drawn sketch.

Google's much-hyped new AI chatbot tool Bard, which has yet to be released to the public, is already being called out for an inaccurate response it produced in a demo this week. In the demo, which was posted by Google on Twitter, a user asks Bard: "What new discoveries from the James Webb Space Telescope can I tell my 9 year old about?" Bard responds with a series of bullet points, including one that reads: "JWST took the very first pictures of a planet outside of our own solar system." According to NASA, however, the first image showing an exoplanet — or any planet beyond our solar system — was actually taken by the European Southern Observatory's Very Large Telescope nearly two decades ago, in 2004. Shares in Google's parent company Alphabet fell 7.7% Wednesday, wiping $100 billion off its market value, after the inaccurate response from Bard was first reported by Reuters. Bard's blunder highlights the challenge for Google as it races to integrate the same AI technology that underpins Microsoft-backed ChatGPT into its core search engine. In trying to keep pace with what some think could be a radical change spurred by conversational AI in how people search online, Google now risks upending its search engine's reputation for surfacing reliable information. Like ChatGPT, Bard is built on a large language model, which is trained on vast troves of data online in order to generate compelling responses to user prompts. Experts have long warned that these tools have the potential to spread inaccurate information. In an apparent attempt to address that concern, Google previously said Bard would first be opened up to "trusted testers" this week, with plans to make it available to the public in the coming weeks. "This highlights the importance of a rigorous testing process, something that we're kicking off this week with our Trusted Tester program," a Google spokesperson told CNN in a statement Wednesday about the factual error. "We'll combine external feedback with our own internal testing to make sure Bard's responses meet a high bar for quality, safety and groundedness in real-world information."  Shares for Google-parent Alphabet fell as much as 8% in midday trading Wednesday after the inaccurate response from Bard was first reported by Reuters.  Google unveiled Bard earlier this week as part of an apparent bid to compete with the viral success of ChatGPT, which has been used to generate essays, song lyrics and responses to questions that one might previously have searched for on Google. ChatGPT's meteoric rise in popularity has reportedly prompted Google's management to declare a "code red" situation for its search product.   On Wednesday, Google held an event in its Paris office where the tech giant detailed plans to use AI technology to radically change how people search for information online. Google's event came one day after rival Microsoft announced a revamped version of Bing powered by a more advanced version of the AI used by ChatGPT. (Microsoft is investing billions in OpenAI, the company behind ChatGPT.) In the presentation Wednesday, a Google executive teased plans to use this technology to offer more complex and conversational responses to queries, including providing bullet points ticking off the best times of year to see various constellations and also offering pros and cons for buying an electric vehicle. The executive said AI technology would pave the way for the "next frontier of our information products.


Microsoft on Thursday outlined its plans to bring artificial intelligence to its most recognizable productivity tools, including Outlook, PowerPoint, Excel and Word, with the promise of changing how millions do their work every day. At an event on Thursday, the company announced that Microsoft 365 users will soon be able to use what the company is calling an AI "Co-pilot," which will help edit, summarize, create and compare documents. But don't call it Clippy. The new features, which are built on the same technology that underpins ChatGPT, are far more powerful (and less anthropomorphized) than its wide-eyed, paperclip-shaped predecessor. With the new features, users will be able to transcribe meeting notes during a Skype call, summarize long email threads to quickly draft suggested replies, request to create a specific chart in Excel, and turn a Word document into a PowerPoint presentation in seconds. Microsoft is also introducing a concept called Business Chat, an agent that essentially rides along with the user as they work and tries to understand and make sense of their Microsoft 365 data. The agent will know what's in a user's email and on their calendar for the day as well as the documents they've been working on, the presentations they've been making, the people they're meeting with, and the chats happening on their Teams platform, according to the company. Users can then ask Business Chat to do tasks such as write a status report by summarizing all of the documents across platforms on a certain project, and then draft an email that could be sent to their team with an update.  Microsoft's announcement comes a month after it brought similar AI-powered features to Bing and amid a renewed arms race in the tech industry to develop and deploy AI tools that can change how people work, shop and create. Earlier this week, rival Google announced it is also bringing AI to its productivity tools, including Gmail, Sheets and Docs. The news also comes two days after OpenAI, the company behind Microsoft's artificial intelligence technology and the creator of ChatGPT, unveiled its next-generation model, GPT-4. The update has stunned many users in early tests and a company demo with its ability to draft lawsuits, pass standardized exams and build a working website from a hand-drawn sketch.  OpenAI said it added more "guardrails" to keep conversations on track and has worked to make the tool less biased. But the update, and the moves by larger tech companies to integrate this technology, could add to challenging questions around how AI tools can upend professions, enable students to cheat, and shift our relationship with technology. Microsoft's new Bing browser has already been using GPT-4, for better or worse. A Microsoft spokesperson said 365 users accessing the new AI tools should be reminded the technology is a work in progress and information will need to be double checked. Although OpenAI has made vast improvements to its latest model, GPT-4 has similar limitations to previous versions. The company said it can still make "simple reasoning errors" or be "overly gullible in accepting obvious false statements from a user," and does not fact check.  Still, Microsoft believes the changes will improve the experience of people at work in a significant way by allowing them to do tasks easier and less tedious, freeing them up to be more analytical and creative. 
                    

Imagine if Siri could write you a college essay, or Alexa could spit out a movie review in the style of Shakespeare. OpenAI last week opened up access to ChatGPT, an AI-powered chatbot that interacts with users in an eerily convincing and conversational way. Its ability to provide lengthy, thoughtful and thorough responses to questions and prompts -- even if inaccurate -- has stunned users, including academics and some in the tech industry.  The tool quickly went viral. On Monday, Open AI's co-founder Sam Altman, a prominent Silicon Valley investor, said on Twitter that ChatGPT crossed one million users. It also captured the attention of some prominent tech leaders, such as Box CEO Aaron Levie.  "There's a certain feeling that happens when a new technology adjusts your thinking about computing. Google did it. Firefox did it. AWS did it. iPhone did it. OpenAI is doing it with ChatGPT," Levie said on Twitter. But as with other AI-powered tools, it also poses possible concerns, including for how it could disrupt creative industries, perpetuate biases and spread misinformation.  What is ChatGPT? ChatGPT is a large language model trained on a massive trove of information online to create its responses. It comes from the same company behind DALL-E, which generates a seemingly limitless range of images in response to prompts from users. It's also the next iteration of text-generator GPT-3. After signing up for ChatGPT, users can ask the AI system to field a range of questions, such as "Who was the president of the United States in 1955," or summarize difficult concepts into something a second grader could understand. It'll even tackle open-ended questions, such as "What's the meaning of life?" or "What should I wear if it's 40 degrees out today?" "It depends on what activities you plan to do. If you plan to be outside, you should wear a light jacket or sweater, long pants, and closed-toe shoes," ChatGPT responded. "If you plan to be inside, you can wear a t-shirt and jeans or other comfortable clothing." But some users are getting very creative. How people are using it One person asked the chatbot to rewrite the 90s hit song, "Baby Got Back," in the Style of "The Canterbury Tales;" another wrote a letter to remove a bad account from a credit report (rather than using a credit repair lawyer). Other colorful examples including asking for fairy-tale inspired home décor tips and giving it an AP English exam question (it responded with a 5 paragraph essay about Wuthering Heights.) In a blog post last week, OpenAI said the "format makes it possible for the tool to answer follow-up questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests."  As of Monday morning, the page to try ChatGPT was down, citing "exceptionally high demand." "Please hang tight as we work on scaling our systems," the message said. (It now appears to be back online). Possible issues While ChatGPT successfully fielded a variety of questions submitted by CNN, some responses were noticeably off. In fact, Stack Overflow -- a Q&A platform for coders and programmers -- temporarily banned users from sharing information from ChatGPT, noting that it's  "substantially harmful to the site and to users who are asking or looking for correct answers." Beyond the issue of spreading incorrect information, the tool could also threaten some written professions, be used to explain problematic concepts, and as with all AI tools, perpetuate biases based on the pool of data on which it's trained. Typing a prompt involving a CEO, for example, could prompt a response assuming that the individual is white and male, for example. "While we've made efforts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or exhibit biased behavior," Open AI said on its website. "We're using the Moderation API to warn or block certain types of unsafe content, but we expect it to have some false negatives and positives for now. We're eager to collect user feedback to aid our ongoing work to improve this system." Still, Lian Jye Su, a research director at market research firm ABI Research, warns the chatbot is operating "without a contextual understanding of the language."  "It is very easy for the model to give plausible-sounding but incorrect or nonsensical answers," he said. "It guessed when it was supposed to clarify and sometimes responded to harmful instructions or exhibited biased behavior. It also lacks regional and country-specific understanding." At the same time, however, it does provide a glimpse into how companies may be able to capitalize on developing more robust virtual assistance, as well as patient and customer care solutions.  While the DALL-E tool is free, it does put a limit on the number of prompts a user can do before having to pay. When Elon Musk, a co-founder of OpenAI, recently asked Altman on Twitter about the average cost per ChatGPT chat, Altman said: "We will have to monetize it somehow at some point; the compute costs are eye-watering.
                    

Google is officially set to confront OpenAI's ChatGPT — and soon.  The tech titan, which has had a stranglehold on internet search for as long as most web users can remember, formally announced Monday that it will roll out Bard, its experimental conversational AI service, in the "coming weeks." The announcement comes just a day before Microsoft, which is working to integrate ChatGPT-like technology into its products, including its search engine Bing, is set to hold an event with OpenAI at its Washington state headquarters.  "The internet search wars are back," wrote the Financial Times' Richard Waters in a piece published Monday, noting that AI has "opened the first new front in the battle for search dominance since Google fended off a concerted challenge from Microsoft's Bing more than a decade ago." A version of this article first appeared in the "Reliable Sources" newsletter. Sign up for the daily digest chronicling the evolving media landscape here. But the rapid emergence of the technology has also raised serious ethical questions, especially since it is being taken to market at a breakneck speed. "We are reliving the social media era," said Beena Ammanath, who leads Trustworthy Tech Ethics at Deloitte and is the executive director of the Global Deloitte AI Institute. Ammanath said that "unintended consequences" accompany every new technology and reluctantly expressed confidence that it too will occur with AI chatbots, unless significant precautions are taken. For now, she doesn't see the guardrails in place to rein in the nascent technology. Instead, Ammanath equated what is currently transpiring with the swift deployment of AI as companies "building Jurassic Park, putting some danger signs on the fences, but leaving all the gates open." Yes, there is some acknowledgment about the dangers the technology poses. But it's not enough, given the risks.  Ammanath stressed that computer scientists working on AI have yet to solve for bias, a years-long problem, as well as other worrisome issues that plague the technology. One major problem is that AI bots cannot separate truth from fantasy. "The challenge with new language models is they blend fact and fiction," Ammanath told me. "It spreads misinformation effectively. It cannot understand the content. So it can spout out completely logical sounding content, but incorrect. And it delivers it with complete confidence." That's effectively what happened last month when CNET was forced to issue corrections on a number of articles, including some that it described as "substantial," after using an AI-powered tool to help the news outlet write dozens of stories. And in its wake, other outlets like BuzzFeed, are already embracing the robot-writing technology to help it generate content and quizzes. "This is a new dimension that generative AI has brought in," Ammanath added. In announcing that Google will roll out its AI soon, chief executive Sundar Pichai stressed that "it's critical that we bring experiences rooted in these models to the world in a bold and responsible way." And Pichai underscored that Google is "committed to developing AI responsibly." But it's hard to deny that the company, under tremendous pressure from investors after ChatGPT stormed onto the scene, is not rushing to deploy its product to the market as quickly as possible. In an internal note to staff, Pichai himself said all hands are on deck and that the company will be "enlisting every Googler to help shape Bard and contribute through a special company-wide" event he said will have "the spirit of an internal hackathon." "We've been approaching this effort with an intensity and focus that reminds me of early Google," Pichai wrote, "so thanks to everyone who has contributed." But its clear that both Google and Microsoft, some of the most valuable and pioneering companies on the web, understand well that AI technology has the power to reshape the world as we know it. The only question is will they follow Silicon Valley's "move fast and break things" maxim that has caused so much turmoil in the past?
                    

Microsoft's public demo last week of an AI-powered revamp of Bing appears to have included several factual errors, highlighting the risk the company and its rivals face when incorporating this new technology into search engines.  At the Bing demo at Microsoft headquarters, the company showed off how integrating artificial intelligence features from the company behind ChatGPT would empower the search engine to provide more conversational and complex search results. The demo included a pros and cons list for products, such as vacuum cleaners; an itinerary for a trip to Mexico City; and the ability to quickly compare corporate earnings results. But it apparently failed to differentiate between the types of vacuums and even made up information about certain products, according to an analysis of the demo this week from independent AI researcher Dmitri Brereton. It also missed relevant details (or fabricated certain information) for the bars it referenced in Mexico City, according to Brereton. In addition, Brereton found it inaccurately stated the operating margin for the retailer Gap, and compared it to a set of Lululemon results that were not factually correct.  "We're aware of this report and have analyzed its findings in our efforts to improve this experience," Microsoft said in a statement. "We recognize that there is still work to be done and are expecting that the system may make mistakes during this preview period, which is why the feedback is critical so we can learn and help the models get better." The company also said thousands of users have interacted with the new Bing since the preview launched last week and shared their feedback, allowing the model to "learn and make many improvements already."  The discovery of Bing's apparent mistakes comes just days after Google was called out for an error made in its public demo last week of a similar AI-powered tool. Google's shares lost $100 billion in value after the error was reported. (Shares of Microsoft were essentially flat on Tuesday.) In the wake of the viral success of ChatGPT, an AI chatbot that can generate shockingly convincing essays and responses to user prompts, a growing number of tech companies are racing to deploy similar technology in their products. But it comes with risks, especially for search engines, which are intended to surface accurate results.  Generative AI systems, which are algorithms that are trained on vast amounts of data online to create new content, are notoriously unreliable, experts say. Laura Edelson, a computer scientist and misinformation researcher at New York University, previously told CNN, "there's a big difference between an AI sounding authoritative and it actually producing accurate results." CNN also conducted a series of tests this week that showed Bing sometimes struggles with accuracy. When asked, "What were Meta's fourth quarter results?" the Bing AI feature gave a response that said, "according to the press release," and then listed bullet points appearing to state Meta's results. But the bullet points were incorrect. Bing said, for example, that Meta generated $34.12 billion in revenue, when the actual amount was $32.17 billion, and said revenue was up from the prior year when in fact it had declined. In a separate search, CNN asked Bing, "What are the pros and cons of the best baby cribs." In its reply, the Bing feature made a list of several cribs and their pros and cons, largely cited to a similar Healthline article. But Bing stated information that appeared to be attributed to the article that was, in fact, not actually there. For example, Bing said one crib had a "water-resistant mattress pad," but that information was listed nowhere in the article. Microsoft and Google executives have previously acknowledged some of the potential issues with the new AI tools. "We know we wont be able to answer every question every single time," Yusuf Mehdi, Microsoft's vice president and consumer chief marketing officer, said last week. "We also know we'll make our share of mistakes, so we've added a quick feedback button at the top of every search, so you can give us feedback and we can learn.

In just a few months, you'll be able to ask a virtual assistant to transcribe meeting notes during a work call, summarize long email threads to quickly draft suggested replies, quickly create a specific chart in Excel, and turn a Word document into a PowerPoint presentation in seconds.  And that's just on Microsoft's 365 platforms. Over the past week, a rapidly evolving artificial intelligence landscape seemed to leap ahead again. Microsoft and Google each unveiled new AI-powered features for their signature productivity tools and OpenAI introduced its next-generation version of the technology that underpins its viral chatbot tool, ChatGPT. Suddenly, AI tools, which have long operated in the background of many services, are now more powerful and more visible across a wide and growing range of workplace tools.  Google's new features, for example, promise to help "brainstorm" and "proofread" written work in Docs. Meanwhile, if your workplace uses popular chat platform Slack, you'll be able to have its ChatGPT tool talk to colleagues for you, potentially asking it to write and respond to new messages and summarize conversations in channels. OpenAI, Microsoft and Google are at the forefront of this trend, but they're not alone. IBM, Amazon, Baidu and Tencent are working on similar technologies. A long list of startups are also developing AI writing assistants and image generators. The pitch from tech companies is clear: AI can make you more productive and eliminate the grunt work. As Microsoft CEO Satya Nadella put it during a presentation on Thursday, "We believe this next generation of AI will unlock a new wave of productivity growth: powerful copilots designed to remove the drudgery from our daily tasks and jobs, freeing us to rediscover the joy of creation." But the sheer number of new options hitting the market is both dizzying and, as with so much else in the tech industry over the past decade, raises questions of whether they will live up to the hype or cause unintended consequences, including enabling cheating and eliminating the need for certain roles (though that may be the intent of some adopters). Even the promise of greater productivity is unclear. The rise of AI-generated emails, for example, might boost productivity for the sender but decrease it for recipients flooded with longer-than-necessary computer-generated messages. And of course just because everyone has the option to use a chatbot to communicate with colleagues doesn't mean all will chose to do so. Integrating this technology "into the foundational pieces of productivity software that most of us use everyday will have a significant impact on the way we work," said Rowan Curran, an analyst at Forrester. "But that change will not wash over everything and everyone tomorrow — learning how to best make use of these capabilities to enhance and adjust our existing workflows will take time." A rapid change in workplace tools Anyone who has ever used an autocomplete option when typing an email or sending a message has already experienced how AI can speed up tasks. But the new tools promise to go far beyond that. The renewed wave of AI product launches kicked off nearly four months ago when OpenAI released a version of ChatGPT on a limited basis, stunning users with generating human-sounding responses to user prompts, passing exams at prestigious universities and writing compelling essays on a range of topics.  Since then, the technology — which Microsoft made a "multibillion dollar" investment in earlier this year — has only improved. Earlier this week, OpenAI unveiled GPT-4, a more powerful version of the technology that underpins ChatGPT, and which promises to blow previous iterations out of the water. In early tests and a company demo, GPT-4 was used to draft lawsuits, build a working website from a hand-drawn sketch and recreate iconic games such as Pong, Tetris or Snake with very little to no prior coding experience. GPT-4 is a large language model that has been trained on vast troves of online data to generate responses to user prompts. It's the same technology that underpins two new Microsoft features:"Co-pilot," which will help edit, summarize, create and compare documents across its platforms, and Business Chat, an agent that essentially rides along with the user as they work and tries to understand and make sense of their Microsoft 365 data.  The agent will know, for example, what's in a user's email and on their calendar for the day, as well as the documents they've been working on, the presentations they've been making, the people they're meeting with, and the chats happening on their Teams platform, according to the company. Users can then ask Business Chat to do tasks such as write a status report by summarizing all of the documents across platforms on a certain project, and then draft an email that could be sent to their team with an update. Curran said just how much these AI-powered tools will change work depends on the application. For example, a word processing application could help generate outlines and drafts, a slideshow program may help speed along the design and content creation process, and a spreadsheet app should help more users interact with and make data-driven decisions. The latter he believes will make the most significant impact to the workplace in both the short and long-term. The discussion of how these technologies will impact jobs "should focus on job tasks rather than jobs as a whole," he said. Challenges ahead Although OpenAI's GPT-4 update promises fixes to some of its biggest challenges — from its potential to perpetuate biases, sometimes being factually incorrect and responding in an aggressive manner — there's still the possibility for some of these issues to find their way into the workplace, especially when it comes to interacting with others.  Arijit Sengupta, CEO and founder of AI solutions company Aible, said a problem with any large language model is that it tries to please the user and typically accepts the premise of the user's statements.  "If people start gossiping about something, it will accept it as the norm and then start generating content [related to that]," said Sengupta, adding that it could escalate interpersonal issues and turn into bullying at the office.  In a tweet earlier this week, OpenAI CEO Sam Altman wrote the technology behind these systems is "still flawed, still limited, and it still seems more impressive on first use than it does after you spend more time with it." The company reiterated in a blog post that "great care should be taken when using language model outputs, particularly in high-stakes contexts." Arun Chandrasekaran, an analyst at Gartner Research, said organizations will need to educate their users on what these solutions are good at and what their limitations are.  "Blind trust in these solutions is as dangerous as complete lack of faith in the effectiveness of it," Chandrasekaran said. "Generative AI solutions can also make up facts or present inaccurate information from time to time -- and organizations need to be prepared to mitigate this negative impact." At the same time, many of these applications are not up to date (GPT-4's data that it's trained on cuts off around September 2021). The onus will have to be on the users to do everything from double check the accuracy to change the language to reflect the tone they want. It will also be important to get buy-in and support across workplaces for the tools to take off. "Training, education and organizational change management is very important to ensure that employees are supportive of the efforts and the tools are used in the way they were intended to," Chandrasekaran said.
                

Shares in Chinese search giant Baidu rebounded sharply a day after it unveiled ERNIE Bot, its answer to the ChatGPT craze. Its stock soared 14.3% on Friday in Hong Kong, making it the biggest winner in the Hang Seng Index. They also gained 3.8% in New York during US trade Thursday.  A day earlier, Baidu was the biggest loser of the same index. Its Hong Kong shares fell 6.4% after a public demonstration of its bot failed to impress investors. Since February, more than 650 companies had joined the ERNIE ecosystem, CEO Robin Li said during the presentation. The reversal came after the company said more than 30,000 businesses had signed up to test out its chatbot service within two hours of its demonstration. "The high degree of enterprise interest is positive, and we expect Baidu to continue to capture China's enterprise demand for generative AI," Esme Pau, Macquarie's head of China and Hong Kong internet and digital assets, told CNN. She said the company's shares were bouncing back Friday as some users, including analysts, shared positive feedback of their own experiences trying out ERNIE, which suggested the bot had more advanced capabilities. High hopes During the presentation, Baidu showed how its chatbot could generate a company newsletter, come up with a corporate slogan and solve a math riddle. But its stock slumped on Thursday because the demo was "pre-recorded, and not live, which makes investors skeptical about the robustness of the ERNIE Bot," according to Pau. Baidu's demonstration also came just days after the launch of GPT-4, which "raised the bar for ERNIE," she added. GPT-4 is the latest version of the artificial intelligence technology behind ChatGPT. The service has impressed users this week with its ability to simplify coding, rapidly create a website from a simple sketch and pass exams with high marks. Pau noted that Baidu's shares were already "down modestly" before showing off its software on Thursday, highlighting pressure from investors who had raised expectations following the GPT-4 launch. "ERNIE also does not have the [same] multilingual capability as GPT-4, and has yet to improve for English queries," she said. "Also, the ERNIE launch did not provide sufficient quantifiable metrics compared to the GPT-4 launch earlier this week." Like ChatGPT, ERNIE is based on a language model, which is trained on vast troves of data online in order to generate compelling responses to user prompts. Li said Baidu's expectations for ERNIE were "close to ChatGPT, or even GPT-4." But he acknowledged the software was "not perfect yet," adding it was being launched first to enterprise users. The service is not yet available to the public.  Competition heating up Baidu announced its chatbot last month. Some critics say the service will add fuel to an existing US-China rivalry in emerging technologies. Li tried to shake off that comparison during the launch, saying the bot "is not a tool for the confrontation between China and the United States in science and technology, but a product of generations of Baidu technicians chasing the dream of changing the world with technology."  "It is a brand new platform for us to serve hundreds of millions of users and empower thousands of industries," he said.  Baidu says its service stands out because of its advanced grasp of Chinese queries, as well as its ability to generate different types of responses. "ERNIE Bot can produce text, images, audio and video given a text prompt, and is even capable of delivering voice in several local dialects such as the Sichuan dialect," the company said in a statement.  By comparison, GPT-4 is also able to analyze photos, but currently only generates text responses, according to its developer, OpenAI. Baidu isn't the only Chinese firm working on such technology. Last month, Alibaba announced plans to launch its own ChatGPT-style tool, adding to the list of tech giants jumping on the chatbot bandwagon. So far, Baidu has a first mover advantage in the space in China, according to analysts.  "Our view is ERNIE is three to six months ahead of its potential contenders," said Pau. Baidu also announced a milestone in its transportation business on Friday, saying it is the first operator in Beijing to be allowed to provide fully driverless ride-hailing services in the city. The company isn't allowed to start charging passengers in the capital for the new service yet. Previously, it was required to keep a driver in the front passenger seat to take over in case of emergencies.  Baidu runs fully driverless commercial taxi services in the cities of Chongqing and Wuhan. 
                    

In the first day after it was unveiled, GPT-4 stunned many users in early tests and a company demo with its ability to draft lawsuits, pass standardized exams and build a working website from a hand-drawn sketch.  On Tuesday, OpenAI announced the next-generation version of the artificial intelligence technology that underpins its viral chatbot tool, ChatGPT. The more powerful GPT-4 promises to blow previous iterations out of the water, potentially changing the way we use the internet to work, play and create. But it could also add to challenging questions around how AI tools can upend professions, enable students to cheat, and shift our relationship with technology. GPT-4 is an updated version of the company's large language model, which is trained on vast amounts of online data to generate complex responses to user prompts. It is now available via a waitlist and has already made its way into some third-party products, including Microsoft's new AI-powered Bing search engine. Some users with early access to the tool are sharing their experiences and highlighting some of its most compelling use cases. Here's a closer look at the potential of GPT-4: Analyzing more than text At its core, the biggest change to GPT-4 is its ability to work with photos that users upload.  One of the most jaw-dropping use cases so far came from an OpenAI video demo that showed how a drawing could be turned into a functional website within minutes. The demonstrator uploaded the picture into GPT-4 and then pasted the resulting code into a preview that showed how it could be a working website.  In its announcement, OpenAI also showed how GPT-4 was asked to explain a joke from a series of images — which featured a smartphone with the wrong charger — and described why it was funny. While it might sound straightforward, dissecting a joke is more complicated for artificial intelligence tools to pick up on because of needed context.  In another test, The New York Times showed GPT-4 a picture of the interior of a refrigerator and prompted it to come up with a meal based on the ingredients. The photos feature isn't live yet, but OpenAI is expected to roll it out in the upcoming weeks.  Coding made even easier Some early GPT-4 users with very little to no prior coding knowledge have also used it to recreate iconic games such as Pong, Tetris or Snake after following step-by-step instructions provided by the tool on how to do so. Others have made their own original games. (GPT-4 can write code in all major programming languages, according to OpenAI.) "The powerful language capabilities of GPT-4 will be used for everything from storyboarding, character creation to gaming content creation," said Arun Chandrasekaran, an analyst at Gartner Research. "This could give rise to more independent gaming providers in the future. But beyond the game itself, GPT-4 and similar models can be used for creating marketing content around game previews, generating news articles and even moderating gaming discussion boards." Similar to gaming, GPT-4 could change the way people develop apps. One user on Twitter said they made a simple drawing app in minutes, while another claimed to have coded an app that recommends five new movies every day, along with providing trailers and details on where to watch them. "Coding is like learning how to drive — as long as the beginner gets some guidance, anyone can code," said Lian Jye Su, an analyst at ABI Research. "AI can be a good teacher." Passing tests with flying colors Although OpenAI said the update is "less capable" than humans in many real-world scenarios, it exhibits "human-level performance" on various professional and academic tests. The company said GPT-4 recently passed a simulated law school bar exam with a score around the top 10% of test takers. By contrast, the prior version, GPT-3.5, scored around the bottom 10%. The latest version also performed strongly on the LSAT, GRE, SATs and many AP exams, according to OpenAI. In January, ChatGPT made headlines for its ability to pass prestigious graduate-level exams, such as one from University of Pennsylvania's Wharton School of Business, but not with particularly high marks. The company said it spent months using lessons from its testing program and ChatGPT to improve the system's accuracy and ability to stay on topic.  Providing more precise responses Compared to the prior version, GPT-4 is able to produce longer, more detailed and more reliable written responses, according to the company.  The latest version can now give responses up to 25,000 words, up from about 4,000 previously, and can provide detailed instructions for even the most unique scenarios, ranging from how to clean a piranha's fish tank to extracting the DNA of a strawberry. One early user said it provided in-depth suggestions for pickup lines based on a question listed on a dating profile. Streamlining work across various industries Joshua Browder, CEO of legal services chatbot DoNotPay, said his company is already working on using the tool to generate "one click lawsuits" to sue robocallers, in an early indication of the vast potential for GPT-4 to change how people work across industries. "Imagine receiving a call, clicking a button, [the] call is transcribed and 1,000 word lawsuit is generated. GPT-3.5 was not good enough, but GPT-4 handles the job extremely well," Browder tweeted. Meanwhile, Jake Kozloski, CEO of dating site Keeper, said his company is using the tool to better match its users. According to Su at ABI Research, it's possible we'll also see major advancements in "connected car [dashboards], remote diagnosis in healthcare, and other AI applications that were previously not possible."  A work in progress Although the company has made vast improvements to its AI model, GPT-4 has similar limitations to previous versions. OpenAI said the technology lacks knowledge of events that occurred before its data set cuts off (September 2021) and does not learn from its experience. It can also make "simple reasoning errors" or be "overly gullible in accepting obvious false statements from a user," and not double-check work, the company said. Gartner's Chandrasekaran said this is also reflective of many AI models today. "Let us not forget that these AI models aren't perfect," Chandrasekaran said. "They can produce inaccurate information from time to time and can be black-box in nature."  For now, OpenAI said GPT-4 users should exercise caution and use "great care" particularly "in high-stakes contexts."
                    

                               
